import os
import json
from dotenv import load_dotenv
from tqdm import tqdm
from langchain_text_splitters import RecursiveCharacterTextSplitter
from generate_data import StoryChunkScorer

def main():
    # åŠ è½½ .env ä¸­çš„ç¯å¢ƒå˜é‡
    load_dotenv()
    os.environ["DEEPSEEK_API_KEY"] = os.getenv("DEEPSEEK_API_KEY")

    scorer = StoryChunkScorer()
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " "]
    )

    data_path = './doc/'
    output_path = 'finetune_dataset.jsonl'

    all_files = [f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f))]

    with open(output_path, 'w', encoding='utf-8') as out_f:
        for filename in tqdm(all_files, desc="ğŸ“„ å¤„ç†æ–‡ä»¶"):
            file_path = os.path.join(data_path, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
                chunks = splitter.split_text(text)

                for chunk in tqdm(chunks, leave=False, desc=f"ğŸ” {filename}"):
                    result_list = scorer.run_workflow(chunk)
                    if not result_list:
                        continue

                    for item in result_list:
                        try:
                            record = {
                                "prompt": item["prompt"].strip(),
                                "completion": item["completion"].strip()
                            }
                            out_f.write(json.dumps(record, ensure_ascii=False) + "\n")
                        except Exception as e:
                            print(f"âš ï¸ å†™å…¥å¤±è´¥ï¼š{e}")

if __name__ == "__main__":
    main()
